import os
import re
import time
import json
from PIL import Image
import base64
import subprocess
import mimetypes
import logging
import asyncio
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional
from io import BytesIO
import tempfile
import shutil
from fastapi.responses import FileResponse, PlainTextResponse
import os
import tempfile
import subprocess
import logging
from fastapi import UploadFile
from PyPDF2 import PdfReader
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel
import uvicorn
from io import BytesIO
import pdfplumber
from pdf2image import convert_from_bytes
import mimetypes
from openai import OpenAI

from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib.pagesizes import A4
from reportlab.lib.units import mm

import spacy.cli
import spacy
from collections import Counter
from fastapi.staticfiles import StaticFiles
from fastapi.responses import RedirectResponse
from dotenv import load_dotenv
import unicodedata
import string

load_dotenv()

OPENAI_MODEL_OCR = os.getenv("OPENAI_MODEL_OCR", "gpt-4o-mini")
OPENAI_MODEL_CLS = os.getenv("OPENAI_MODEL_CLS", "gpt-4o-mini")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "YOUR_API_KEY_HERE")

OPENAI_MODEL_OCR_PRIMARY = os.getenv("OPENAI_MODEL_OCR", "gpt-4o-mini")
OPENAI_MODEL_OCR_FALLBACK = os.getenv("OPENAI_MODEL_OCR_FALLBACK", "gpt-4o")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("ClassifyMyNotes")

app = FastAPI(title="ClassifyMyNotes API", version="2.0.1")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.mount("/web", StaticFiles(directory="static", html=True), name="web")


@app.get("/")
def root():
    return RedirectResponse(url="/web/")


client = OpenAI(api_key=OPENAI_API_KEY)


def chat_call_with_retry(**kwargs):
    backoff = 2.0
    for i in range(6):
        try:
            return client.chat.completions.create(**kwargs)
        except Exception as e:
            msg = str(e).lower()
            if "rate limit" in msg or "429" in msg or "temporarily" in msg:
                sleep_s = backoff * (i + 1)
                logger.warning(f"Retry {i + 1}: {e} -> sleep {sleep_s:.1f}s")
                time.sleep(sleep_s)
                continue
            raise


def strip_latex_preamble(text: str) -> str:
    text = re.sub(r'\\documentclass.*', '', text)
    text = re.sub(r'\\usepackage.*', '', text)
    text = re.sub(r'\\begin\{document\}', '', text)
    text = re.sub(r'\\end\{document\}', '', text)
    return text.strip()


def load_spacy_ru():
    import spacy
    try:
        return spacy.load("ru_core_news_sm")
    except OSError:
        import spacy.cli
        logger.info("–°–∫–∞—á–∏–≤–∞—é spacy ru_core_news_sm...")
        spacy.cli.download("ru_core_news_sm")
        return spacy.load("ru_core_news_sm")


nlp = load_spacy_ru()


def latex_escape(text: str) -> str:
    """
    –≠–∫—Ä–∞–Ω–∏—Ä—É–µ—Ç —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã LaTeX, —á—Ç–æ–±—ã –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –ø–∞–¥–∞–ª –ø—Ä–∏ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏.
    """
    replacements = {
        "\\": r"\\",
        "{": r"\{",
        "}": r"\}",
        "$": r"\$",
        "&": r"\&",
        "#": r"\#",
        "_": r"\_",
        "%": r"\%",
        "~": r"\textasciitilde{}",
        "^": r"\^{}"
    }
    for orig, repl in replacements.items():
        text = text.replace(orig, repl)
    return text


@dataclass
class ClassifiedSegment:
    text: str
    category: str
    confidence: float
    start_pos: int
    end_pos: int


class ChatMessage(BaseModel):
    message: str
    context: Optional[str] = None


class ChatResponse(BaseModel):
    response: str
    suggestions: List[str]


class ProcessingResult(BaseModel):
    success: bool
    extracted_text: str
    segments: List[Dict]
    highlighted_html: str
    summary: Dict
    file_info: Dict
    latex_source: Optional[str] = None




def _img_to_png_bytes(img: Image.Image, scale: float = 1.0, quality: int = 95) -> bytes:
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ PNG —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
    if scale != 1.0:
        w, h = img.size
        new_w, new_h = int(w * scale), int(h * scale)
        img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)

    if img.mode not in ('RGB', 'L'):
        img = img.convert('RGB')

    buf = BytesIO()
    img.save(buf, format="PNG", optimize=True, compress_level=1)
    return buf.getvalue()


def _tile_image(img: Image.Image, grid=(2, 2), overlap=10) -> List[bytes]:
    """–†–∞–∑–±–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞ —Ç–∞–π–ª—ã —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º."""
    W, H = img.size
    cols, rows = grid
    tiles = []
    w = W // cols
    h = H // rows

    for r in range(rows):
        for c in range(cols):
            left = max(0, c * w - (overlap if c > 0 else 0))
            top = max(0, r * h - (overlap if r > 0 else 0))
            right = min(W, (c + 1) * w + (overlap if c + 1 < cols else 0))
            bottom = min(H, (r + 1) * h + (overlap if r + 1 < rows else 0))

            crop = img.crop((left, top, right, bottom))
            tiles.append(_img_to_png_bytes(crop, scale=1.0))

    return tiles


def as_data_url_from_bytes(data: bytes, mime_type: str = "image/jpeg") -> str:
    b64 = base64.b64encode(data).decode("utf-8")
    return f"data:{mime_type};base64,{b64}"


def safe_truncate(s: str, limit=12000):
    return s if len(s) <= limit else s[:limit] + "\n...[truncated]..."


def fix_latex_commands(text: str) -> str:
    """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –æ—à–∏–±–∫–∏ –≤ LaTeX –∫–æ–º–∞–Ω–¥–∞—Ö"""
    text = re.sub(r'\\\\\s+([A-Za-z])', r'\\\1', text)
    text = re.sub(r'\\\\(\s*[a-zA-Z])', r'\\\\\n\1', text)
    return text





def _strip_latex_fences(s: str) -> str:
    """–£–±–∏—Ä–∞–µ–º markdown code fences –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —Ç–∞–∫–æ–µ –≤–µ—Ä–Ω—É–ª–∞"""
    s = s.strip()
    s = re.sub(r"^```(?:latex)?\s*", "", s, flags=re.MULTILINE)
    s = re.sub(r"\s*```$", "", s, flags=re.MULTILINE)
    return s.strip()


def validate_and_fix_latex(content: str) -> str:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ –≤ LaTeX –∫–æ–¥–µ
    """
    content = re.sub(r'\$([^$]*)\$', lambda m: f'${m.group(1).strip()}$', content)

    content = re.sub(r'\\begin\{equation\}\s*\\end\{equation\}', '', content)

    def fix_align(match):
        align_content = match.group(1)
        lines = align_content.split('\n')
        fixed_lines = []
        for i, line in enumerate(lines):
            line = line.strip()
            if line and not line.endswith('\\\\') and i < len(lines) - 1:
                line += ' \\\\'
            fixed_lines.append(line)
        return f'\\begin{{align}}\n{chr(10).join(fixed_lines)}\n\\end{{align}}'

    content = re.sub(r'\\begin\{align\}(.*?)\\end\{align\}', fix_align, content, flags=re.DOTALL)
    return content


_CYR_CONFUSABLES = {
    ord("ƒ∏"): "–∫",
    ord("ƒ±"): "–∏",
    0x2019: "'",
    ord("`"): "'",
    0x201C: '"',
    0x201D: '"',
}




def _text_looks_gibberish(s: str) -> bool:
    """–ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º—É—Å–æ—Ä –ø–æ—Å–ª–µ OCR."""
    if not s or len(s.strip()) < 10:
        return True

    s_clean = s.strip()

    bad_markers = ("ÔøΩ", "‚ñ°", "‚ñØ", "ÔøΩ")
    if any(b in s_clean for b in bad_markers):
        return True

    meaningful_chars = sum(1 for ch in s_clean if ch.isalnum() or ch.isspace() or ch in ".,!?;:-()[]{}¬´¬ª\"'")
    if len(s_clean) > 0:
        meaningful_ratio = meaningful_chars / len(s_clean)
        if meaningful_ratio < 0.7:
            return True

    if len(set(s_clean)) < max(3, len(s_clean) // 20):
        return True

    weird_patterns = [
        r'[a-zA-Z]{20,}',
        r'[A-Z]{10,}',
        r'\d{15,}',
        r'[^\w\s]{10,}',
    ]

    import re
    for pattern in weird_patterns:
        if re.search(pattern, s_clean):
            return True

    mixed_script_words = 0
    words = re.findall(r'\w{3,}', s_clean)
    for word in words:
        has_latin = bool(re.search(r'[A-Za-z]', word))
        has_cyrillic = bool(re.search(r'[–ê-–Ø–∞-—è–Å—ë]', word))
        if has_latin and has_cyrillic:
            mixed_script_words += 1

    if len(words) > 0 and mixed_script_words / len(words) > 0.3:
        return True

    return False


OCR_MAX_TOKENS_DEFAULT = 4096
OCR_MAX_TOKENS_HIGH = 8192
OCR_MAX_TOKENS_ULTRA = 16384

OCR_TOKENS_CONFIG = {
    "basic": 4096,
    "high_quality": 8192,
    "ultra_quality": 16384,
    "tiling": 6144,
    "fallback": 12288,
}




async def ocr_page_smart(img: Image.Image, page_num: int = 1,
                                  adaptive_tokens: bool = True) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è OCR —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –≤—ã–±–æ—Ä–æ–º —Ç–æ–∫–µ–Ω–æ–≤."""
    logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º —É–º–Ω—ã–π OCR —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} (–∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: {adaptive_tokens})...")

    img_area = img.size[0] * img.size[1]

    if adaptive_tokens:
        if img_area > 3000000:
            base_tokens = OCR_TOKENS_CONFIG["ultra_quality"]
            logger.info(f"–ë–æ–ª—å—à–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ({img_area} –ø–∏–∫—Å–µ–ª–µ–π) ‚Üí {base_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
        elif img_area > 1500000:
            base_tokens = OCR_TOKENS_CONFIG["high_quality"]
            logger.info(f"–°—Ä–µ–¥–Ω–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ({img_area} –ø–∏–∫—Å–µ–ª–µ–π) ‚Üí {base_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
        else:
            base_tokens = OCR_TOKENS_CONFIG["basic"]
            logger.info(f"–ù–µ–±–æ–ª—å—à–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ({img_area} –ø–∏–∫—Å–µ–ª–µ–π) ‚Üí {base_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
    else:
        base_tokens = OCR_TOKENS_CONFIG["high_quality"]

    strategies = []

    png_1x = _img_to_png_bytes(img, scale=1.0)
    strategies.append((png_1x, False, OPENAI_MODEL_OCR_PRIMARY, "1.0x Plain Primary", base_tokens))

    png_13x = _img_to_png_bytes(img, scale=1.3)
    strategies.append((png_13x, False, OPENAI_MODEL_OCR_PRIMARY, "1.3x Plain Primary", base_tokens + 2048))

    strategies.append((png_1x, True, OPENAI_MODEL_OCR_PRIMARY, "1.0x LaTeX Primary", base_tokens))

    strategies.append((png_13x, True, OPENAI_MODEL_OCR_FALLBACK, "1.3x LaTeX Fallback",
                       OCR_TOKENS_CONFIG["ultra_quality"]))

    png_15x = _img_to_png_bytes(img, scale=1.5)
    strategies.append((png_15x, False, OPENAI_MODEL_OCR_FALLBACK, "1.5x Plain Fallback",
                       OCR_TOKENS_CONFIG["fallback"]))

    best_result = ""
    best_quality = 0.0

    for image_bytes, latex_mode, model, strategy_name, max_tokens in strategies:
        try:
            logger.info(f"–ü—Ä–æ–±—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é: {strategy_name} ({max_tokens} —Ç–æ–∫–µ–Ω–æ–≤)")
            text = await _ocr_trial(
                image_bytes,
                latex_mode=latex_mode,
                model=model,
                page_num=page_num,
                max_tokens=max_tokens
            )

            if not text.strip():
                logger.warning(f"–°—Ç—Ä–∞—Ç–µ–≥–∏—è {strategy_name}: –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                continue

            if _text_looks_gibberish(text):
                logger.warning(f"–°—Ç—Ä–∞—Ç–µ–≥–∏—è {strategy_name}: —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Ö–æ–∂ –Ω–∞ –º—É—Å–æ—Ä")
                continue

            quality = _estimate_text_quality(text)
            logger.info(f"–°—Ç—Ä–∞—Ç–µ–≥–∏—è {strategy_name}: –∫–∞—á–µ—Å—Ç–≤–æ={quality:.2f}, —Å–∏–º–≤–æ–ª–æ–≤={len(text)}")

            if quality > 0.7:
                logger.info(f"‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ {strategy_name}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë")
                return text
            elif quality > best_quality:
                best_result = text
                best_quality = quality

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ {strategy_name}: {e}")
            continue

    if best_quality > 0.3:
        logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –∫–∞—á–µ—Å—Ç–≤–æ–º {best_quality:.2f}")
        return best_result

    logger.info("üîÑ –í—Å–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–∞–ª–∏ –ø–ª–æ—Ö–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø—Ä–æ–±—É–µ–º —Ç–∞–π–ª–∏–Ω–≥ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–æ–∫–µ–Ω–æ–≤...")
    try:
        tiles = _tile_image(img, grid=(2, 2), overlap=20)
        tile_parts = []

        for i, tile_bytes in enumerate(tiles):
            logger.info(f"OCR —Ç–∞–π–ª–∞ {i + 1}/{len(tiles)} –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
            tile_text = await _ocr_trial(
                tile_bytes,
                latex_mode=False,
                model=OPENAI_MODEL_OCR_FALLBACK,
                page_num=f"{page_num}.{i + 1}",
                max_tokens=OCR_TOKENS_CONFIG["tiling"]
            )

            if tile_text.strip() and not _text_looks_gibberish(tile_text):
                tile_parts.append(tile_text.strip())

        if tile_parts:
            result = "\n\n".join(tile_parts)
            quality = _estimate_text_quality(result)
            logger.info(f"–¢–∞–π–ª–∏–Ω–≥ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –∫–∞—á–µ—Å—Ç–≤–æ–º {quality:.2f}")
            if quality > 0.2:
                return result

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Ç–∞–π–ª–∏–Ω–≥–∞ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")

    logger.warning(f"‚ùå –í—Å–µ –º–µ—Ç–æ–¥—ã OCR –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
    return ""


def _estimate_text_quality(text: str) -> float:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞ –æ—Ç 0 –¥–æ 1."""
    if not text or len(text.strip()) < 10:
        return 0.0

    s = text.strip()

    total_chars = len(s)

    letters = sum(1 for ch in s if ch.isalpha())
    letter_ratio = letters / total_chars if total_chars > 0 else 0

    spaces = s.count(' ')
    space_ratio = spaces / total_chars if total_chars > 0 else 0
    optimal_space_ratio = 0.15
    space_score = 1.0 - abs(space_ratio - optimal_space_ratio) / optimal_space_ratio
    space_score = max(0, min(1, space_score))

    unique_chars = len(set(s.lower()))
    diversity_score = min(1.0, unique_chars / 30.0)

    import re
    words = re.findall(r'\b[–∞-—è—ë–ê-–Ø–Åa-zA-Z]{3,}\b', s)
    word_count = len(words)
    word_score = min(1.0, word_count / max(1, total_chars // 10))

    weird_penalty = 0
    if re.search(r'[A-Za-z]{15,}', s):
        weird_penalty += 0.3
    if re.search(r'\d{10,}', s):
        weird_penalty += 0.2
    if re.search(r'[^\w\s]{8,}', s):
        weird_penalty += 0.3

    quality = (letter_ratio * 0.3 + space_score * 0.2 + diversity_score * 0.2 + word_score * 0.3) - weird_penalty
    return max(0.0, min(1.0, quality))





async def extract_pdf_text_smart(file_path: str) -> str:
    """–ö–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF."""
    logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ PDF: {file_path}")

    try:
        reader = PdfReader(file_path)
        total_pages = len(reader.pages)
        logger.info(f"PDF —Å–æ–¥–µ—Ä–∂–∏—Ç {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")

        text_quality_scores = []
        text_pages_data = []

        for page_idx, page in enumerate(reader.pages, 1):
            try:
                extracted = page.extract_text() or ""
                extracted_clean = normalize_text_unicode(extracted.strip())

                if extracted_clean:
                    quality = _estimate_text_quality(extracted_clean)
                    text_quality_scores.append(quality)
                    text_pages_data.append((page_idx, extracted_clean, quality))
                    logger.info(
                        f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_idx}: —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π –∫–∞—á–µ—Å—Ç–≤–æ={quality:.2f}, —Å–∏–º–≤–æ–ª–æ–≤={len(extracted_clean)}")
                else:
                    text_quality_scores.append(0.0)
                    text_pages_data.append((page_idx, "", 0.0))
                    logger.info(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_idx}: —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_idx}: {e}")
                text_quality_scores.append(0.0)
                text_pages_data.append((page_idx, "", 0.0))

        avg_text_quality = sum(text_quality_scores) / len(text_quality_scores) if text_quality_scores else 0
        good_text_pages = sum(1 for score in text_quality_scores if score > 0.5)

        logger.info(f"–°—Ä–µ–¥–Ω–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–ª–æ—è: {avg_text_quality:.2f}")
        logger.info(f"–°—Ç—Ä–∞–Ω–∏—Ü —Å —Ö–æ—Ä–æ—à–∏–º —Ç–µ–∫—Å—Ç–æ–º: {good_text_pages}/{total_pages}")

        final_results = []

        if good_text_pages >= total_pages * 0.7:
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π")

            for page_idx, text, quality in text_pages_data:
                if quality > 0.5:
                    final_results.append((page_idx, text))
                else:
                    logger.info(f"OCR –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_idx} (–ø–ª–æ—Ö–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π)")
                    try:
                        with open(file_path, "rb") as f:
                            pdf_bytes = f.read()


                        images = convert_from_bytes(
                            pdf_bytes,
                            dpi=300,
                            fmt='PNG',
                            first_page=page_idx,
                            last_page=page_idx
                        )

                        if images:
                            ocr_text = await ocr_page_smart(images[0], page_idx)
                            final_results.append((page_idx, ocr_text))
                        else:
                            final_results.append((page_idx, f"[–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_idx}]"))

                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ OCR —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_idx}: {e}")
                        final_results.append((page_idx, f"[–û—à–∏–±–∫–∞ OCR —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_idx}]"))

        else:
            logger.info("–¢–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π –ø–ª–æ—Ö–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø–æ–ª–Ω–æ–º—É OCR")

            try:
                with open(file_path, "rb") as f:
                    pdf_bytes = f.read()

                images = convert_from_bytes(pdf_bytes, dpi=300, fmt='PNG')

                if len(images) != total_pages:
                    logger.warning(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ({len(images)}) != –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å—Ç—Ä–∞–Ω–∏—Ü ({total_pages})")

                for page_idx in range(1, min(len(images), total_pages) + 1):
                    try:
                        img = images[page_idx - 1]
                        ocr_text = await ocr_page_smart(img, page_idx)

                        text_layer_quality = text_quality_scores[page_idx - 1] if page_idx <= len(
                            text_quality_scores) else 0
                        ocr_quality = _estimate_text_quality(ocr_text) if ocr_text else 0

                        if text_layer_quality > 0.3 and text_layer_quality > ocr_quality:
                            _, text_layer_text, _ = text_pages_data[page_idx - 1]
                            final_results.append((page_idx, text_layer_text))
                            logger.info(
                                f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_idx}: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π (–∫–∞—á–µ—Å—Ç–≤–æ {text_layer_quality:.2f} > OCR {ocr_quality:.2f})")
                        else:
                            final_results.append((page_idx, ocr_text))
                            logger.info(
                                f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_idx}: –∏—Å–ø–æ–ª—å–∑—É–µ–º OCR (–∫–∞—á–µ—Å—Ç–≤–æ {ocr_quality:.2f} > —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π {text_layer_quality:.2f})")

                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_idx}: {e}")
                        final_results.append((page_idx, f"[–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_idx}]"))

            except Exception as e:
                logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª–Ω–æ–º OCR: {e}")
                for page_idx, text, quality in text_pages_data:
                    final_results.append(
                        (page_idx, text if quality > 0.1 else f"[–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_idx}]"))

        result_parts = []
        total_extracted_chars = 0

        for page_idx, page_text in sorted(final_results):
            if page_text and page_text.strip():
                result_parts.append(f"% –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_idx}\n{page_text.strip()}")
                total_extracted_chars += len(page_text.strip())
            else:
                result_parts.append(f"% –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_idx}\n[–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç]")

        final_text = "\n\n".join(result_parts)

        logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
        logger.info(f"  - –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
        logger.info(f"  - –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤: {total_extracted_chars}")
        logger.info(f"  - –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {total_extracted_chars // max(1, total_pages)} —Å–∏–º–≤–æ–ª–æ–≤")

        return final_text

    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è PDF: {e}")
        try:
            logger.info("–ü—Ä–æ–±—É–µ–º –±–∞–∑–æ–≤—ã–π OCR –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å—Ä–µ–¥—Å—Ç–≤–æ...")
            return await ocr_pdf_with_gpt(file_path)
        except Exception as e2:
            logger.error(f"–î–∞–∂–µ –±–∞–∑–æ–≤—ã–π OCR –ø—Ä–æ–≤–∞–ª–∏–ª—Å—è: {e2}")
            return f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF: {str(e)}"


async def ocr_pdf_with_gpt(pdf_input, mode: str = "latex") -> str:
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–ª–Ω—ã–π OCR PDF...")

    try:
        if isinstance(pdf_input, (bytes, bytearray)):
            data = pdf_input
        else:
            with open(pdf_input, "rb") as f:
                data = f.read()

        images = convert_from_bytes(data, dpi=300, fmt='PNG')
        logger.info(f"PDF –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

        page_results = []

        semaphore = asyncio.Semaphore(3)

        async def process_page(idx: int, img: Image.Image) -> Tuple[int, str]:
            async with semaphore:
                page_num = idx + 1
                try:
                    text = await ocr_page_smart(img, page_num)
                    quality = _estimate_text_quality(text) if text else 0

                    if quality > 0.3:
                        logger.info(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ (–∫–∞—á–µ—Å—Ç–≤–æ {quality:.2f})")
                        return (page_num, text)
                    else:
                        logger.warning(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ ({quality:.2f}), –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –ø—Ä–æ–±–ª–µ–º–Ω—É—é")
                        return (page_num, f"[–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤]")

                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
                    return (page_num, f"[–û—à–∏–±–∫–∞ OCR: {str(e)}]")

        tasks = [process_page(idx, img) for idx, img in enumerate(images)]
        results = await asyncio.gather(*tasks)

        results.sort(key=lambda x: x[0])

        page_texts = []
        successful_pages = 0

        for page_num, text in results:
            if text and not text.startswith("["):
                page_texts.append(f"% –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}\n{text}")
                successful_pages += 1
            else:
                page_texts.append(f"% –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}\n{text}")

        final_result = "\n\n".join(page_texts)

        logger.info(f"–ü–æ–ª–Ω—ã–π OCR –∑–∞–≤–µ—Ä—à–µ–Ω:")
        logger.info(f"  - –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(images)}")
        logger.info(f"  - –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful_pages}")
        logger.info(f"  - –ò—Ç–æ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {len(final_result)}")

        return final_result

    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–≥–æ OCR: {e}")
        return f"–û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–≥–æ OCR: {str(e)}"


def normalize_cyrillic_confusables(s: str) -> str:
    replacements = {
        "ƒ∏": "–∫",
        "ƒ±": "–∏",
        "'": "'",
        "`": "'",
        """: '"',   
        """: '"',
    }
    for old, new in replacements.items():
        s = s.replace(old, new)
    return s


def normalize_text_unicode(s: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç: —É–±–∏—Ä–∞–µ—Ç markdown-—Ñ–µ–Ω—Å—ã, –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ NFC
    –∏ –∑–∞–º–µ–Ω—è–µ—Ç —á–∞—Å—Ç—ã–µ –ø—Å–µ–≤–¥–æ-–∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã.
    """
    s = s or ""
    try:
        s = _strip_latex_fences(s)
    except NameError:
        pass
    s = unicodedata.normalize("NFC", s)
    s = normalize_cyrillic_confusables(s)
    return s


def _page_is_gibberish(s: str) -> bool:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –º—É—Å–æ—Ä."""
    if not s or len(s.strip()) < 20:
        return True

    s_clean = s.strip()

    if len(s_clean) < 100:
        # –ò—â–µ–º —Ö–æ—Ç—è –±—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ –±—É–∫–≤ –ø–æ–¥—Ä—è–¥
        words = re.findall(r'[–∞-—è—ë–ê-–Ø–Åa-zA-Z]{3,}', s_clean)
        if len(words) >= 2:
            return False

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ "–ø–ª–æ—Ö–∏—Ö" —Å–∏–º–≤–æ–ª–æ–≤
    allowed = set(string.ascii_letters + string.digits + string.whitespace +
                  "–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø" +
                  ".,;:!?-‚Äì‚Äî()[]{}¬´¬ª\"'/%+*=<>|\\_^$‚Ññ¬±~@#&")

    bad_chars = sum(1 for ch in s_clean if ch not in allowed)
    bad_ratio = bad_chars / max(1, len(s_clean))

    if bad_ratio > 0.4:  # –±—ã–ª–æ 0.35, —É–≤–µ–ª–∏—á–∏–ª–∏ –ø–æ—Ä–æ–≥
        return True

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–∫—Å–∞ –ª–∞—Ç–∏–Ω–∏—Ü—ã –∏ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã - –±–æ–ª–µ–µ –º—è–≥–∫–∞—è
    mixed_words = 0
    for w in re.findall(r"\w{3,}", s_clean):  # —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤
        has_lat = bool(re.search(r"[A-Za-z]", w))
        has_cyr = bool(re.search(r"[–ê-–Ø–∞-—è–Å—ë]", w))
        if has_lat and has_cyr:
            mixed_words += 1

    if mixed_words >= 10:  # –±—ã–ª–æ 5, —É–≤–µ–ª–∏—á–∏–ª–∏ –ø–æ—Ä–æ–≥
        return True

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã - –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    cyr_chars = sum(1 for ch in s_clean if "–ê" <= ch <= "—è" or ch in "–Å—ë")
    if len(s_clean) > 200 and cyr_chars < max(10, len(s_clean) // 120):  # –±—ã–ª–æ //80
        return True

    return False


async def ocr_page_with_fallback(img: Image.Image, page_num: int = 1) -> str:
    """OCR —Å fallback –∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º –º–µ—Ç–æ–¥–∞–º –µ—Å–ª–∏ GPT –æ—Ç–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è"""

    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥
    result = await ocr_page_smart(img, page_num, adaptive_tokens=True)

    if not result or len(result.strip()) < 10:
        logger.warning(f"–û—Å–Ω–æ–≤–Ω–æ–π OCR –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã")

        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π
        try:
            png_bytes = _img_to_png_bytes(img, scale=1.0)

            simple_instruction = "Please describe what you see in this image, focusing on any text content."

            resp = chat_call_with_retry(
                model=OPENAI_MODEL_OCR_PRIMARY,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": simple_instruction},
                        {"type": "image_url", "image_url": {"url": as_data_url_from_bytes(png_bytes, "image/png")}}
                    ]
                }],
                max_tokens=4096,
                temperature=0.3
            )

            alternative_text = resp.choices[0].message.content or ""
            if alternative_text and "can't help" not in alternative_text.lower():
                logger.info(
                    f"–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π OCR –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {len(alternative_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                return alternative_text

        except Exception as e:
            logger.error(f"–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π OCR —Ç–æ–∂–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")

    return result


async def _ocr_trial(image_bytes: bytes, *, latex_mode: bool, model: str,
                              lang_hint: str = "—Ä—É—Å—Å–∫–∏–π, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π",
                              page_num: int = 1,
                              max_tokens: int = OCR_MAX_TOKENS_HIGH) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π OCR trial —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º –ª–∏–º–∏—Ç–æ–º —Ç–æ–∫–µ–Ω–æ–≤ –∏ –º—è–≥–∫–∏–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏."""
    try:
        if latex_mode:
            instruction = f"""–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–º–æ–≥–∏—Ç–µ –º–Ω–µ –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç —Å —ç—Ç–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –µ–≥–æ –≤ LaTeX —Ñ–æ—Ä–º–∞—Ç–µ.

–ß—Ç–æ –º–Ω–µ –Ω—É–∂–Ω–æ:
- –ü—Ä–æ—á–∏—Ç–∞—Ç—å –≤–µ—Å—å –≤–∏–¥–∏–º—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}
- –í–µ—Ä–Ω—É—Ç—å —Ç–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ (–±–µ–∑ –ø—Ä–µ–∞–º–±—É–ª—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞)
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LaTeX —Ä–∞–∑–º–µ—Ç–∫—É –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º—É–ª
- –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É: –∑–∞–≥–æ–ª–æ–≤–∫–∏, –∞–±–∑–∞—Ü—ã, —Å–ø–∏—Å–∫–∏

–û—Å–Ω–æ–≤–Ω—ã–µ —è–∑—ã–∫–∏ —Ç–µ–∫—Å—Ç–∞: {lang_hint}

–ï—Å–ª–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–µ—Ä–∞–∑–±–æ—Ä—á–∏–≤—ã, —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ - –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–µ –∏—Ö."""

        else:
            instruction = f"""–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–º–æ–≥–∏—Ç–µ –º–Ω–µ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å —ç—Ç–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

–ß—Ç–æ –º–Ω–µ –Ω—É–∂–Ω–æ:
- –ò–∑–≤–ª–µ—á—å –≤–µ—Å—å –≤–∏–¥–∏–º—ã–π —Ç–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}
- –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
- –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –≤ LaTeX —Ñ–æ—Ä–º–∞—Ç–µ ($...$)
- –ù–µ –¥–æ–±–∞–≤–ª—è—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏–ª–∏ –ø–æ—è—Å–Ω–µ–Ω–∏—è

–û—Å–Ω–æ–≤–Ω—ã–µ —è–∑—ã–∫–∏: {lang_hint}

–ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ—Ä–∞–∑–±–æ—Ä—á–∏–≤–æ, –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–∏ —á–∞—Å—Ç–∏."""

        logger.info(f"OCR —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: –∏—Å–ø–æ–ª—å–∑—É–µ–º {max_tokens} —Ç–æ–∫–µ–Ω–æ–≤ (–º–æ–¥–µ–ª—å: {model})")

        resp = chat_call_with_retry(
            model=model,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": instruction},
                    {"type": "image_url", "image_url": {"url": as_data_url_from_bytes(image_bytes, "image/png")}}
                ]
            }],
            max_tokens=max_tokens,
            temperature=0.1
        )

        text = resp.choices[0].message.content or ""

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–∫–∞–∑—ã
        refusal_patterns = [
            "–Ω–µ –º–æ–≥—É –ø–æ–º–æ—á—å",
            "–Ω–µ –º–æ–≥—É –≤—ã–ø–æ–ª–Ω–∏—Ç—å",
            "–Ω–µ –º–æ–≥—É –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å",
            "sorry, i can't",
            "i can't help",
            "i cannot assist",
            "–ø—Ä–æ—Ç–∏–≤ –ø–æ–ª–∏—Ç–∏–∫",
            "–ø—Ä–æ—Ç–∏–≤ –ø—Ä–∞–≤–∏–ª"
        ]

        text_lower = text.lower()
        if any(pattern in text_lower for pattern in refusal_patterns):
            logger.warning(f"–ú–æ–¥–µ–ª—å –æ—Ç–∫–∞–∑–∞–ª–∞—Å—å –≤—ã–ø–æ–ª–Ω—è—Ç—å OCR –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {text[:100]}")
            return ""

        # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
        if hasattr(resp, 'usage') and resp.usage:
            prompt_tokens = getattr(resp.usage, 'prompt_tokens', 0)
            completion_tokens = getattr(resp.usage, 'completion_tokens', 0)
            total_tokens = getattr(resp.usage, 'total_tokens', 0)
            logger.info(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: prompt={prompt_tokens}, completion={completion_tokens}, "
                        f"total={total_tokens}, –ª–∏–º–∏—Ç={max_tokens}")

            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞
            if completion_tokens >= max_tokens * 0.95:
                logger.warning(f"‚ö†Ô∏è –ü–æ—á—Ç–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤! –í–æ–∑–º–æ–∂–Ω–æ, —Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω. "
                               f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å max_tokens –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")

        if latex_mode:
            text = _strip_latex_fences(text)

        text = normalize_text_unicode(text)

        # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        quality = _estimate_text_quality(text)
        logger.info(f"OCR —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: –∫–∞—á–µ—Å—Ç–≤–æ={quality:.2f}, —Å–∏–º–≤–æ–ª–æ–≤={len(text)}")

        return text

    except Exception as e:
        logger.error(f"OCR trial failed –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
        return ""



async def ocr_image_with_gpt(
        image_data: bytes,
        model: str = OPENAI_MODEL_OCR,
        max_tokens: int = OCR_TOKENS_CONFIG["high_quality"],  # ‚Üê –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –ª–∏–º–∏—Ç
        latex_mode: bool = False
) -> str:
    """OCR –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ OpenAI Vision-–º–æ–¥–µ–ª—å —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏"""
    data_url = as_data_url_from_bytes(image_data)
    logger.info(f"OCR —á–µ—Ä–µ–∑ {model} –≤ —Ä–µ–∂–∏–º–µ {'latex' if latex_mode else 'plain'}...")

    if latex_mode:
        instruction = (
            "–†–∞—Å–ø–æ–∑–Ω–∞–π –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≤–µ—Ä–Ω–∏ –µ–≥–æ –≤ LaTeX —Ñ–æ—Ä–º–∞—Ç–µ.\n"
            "–í–ê–ñ–ù–û: –í–æ–∑–≤—Ä–∞—â–∞–π –¢–û–õ–¨–ö–û —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Ç–µ–∫—Å—Ç, —Ñ–æ—Ä–º—É–ª—ã, –æ–∫—Ä—É–∂–µ–Ω–∏—è), "
            "–ë–ï–ó \\documentclass, \\usepackage, \\begin{document}, \\end{document}.\n"
            "–ò—Å–ø–æ–ª—å–∑—É–π –æ–∫—Ä—É–∂–µ–Ω–∏—è \\begin{equation}, \\begin{align} –¥–ª—è —Ñ–æ—Ä–º—É–ª.\n"
            "–î–ª—è inline —Ñ–æ—Ä–º—É–ª –∏—Å–ø–æ–ª—å–∑—É–π $...$.\n"
            "–í–æ–∑–≤—Ä–∞—â–∞–π —Ç–æ–ª—å–∫–æ LaTeX –∫–æ–¥ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ, –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."
        )
    else:
        instruction = (
            "–ü–µ—Ä–µ–ø–∏—à–∏ –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ, –≤–∫–ª—é—á–∞—è —Ñ–æ—Ä–º—É–ª—ã. "
            "–í–æ–∑–≤—Ä–∞—â–∞–π —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç, –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."
        )

    resp = chat_call_with_retry(
        model=model,
        messages=[{
            "role": "user",
            "content": [
                {"type": "text", "text": instruction},
                {"type": "image_url", "image_url": {"url": data_url}}
            ]
        }],
        max_tokens=max_tokens,
        temperature=0.0
    )

    # –õ–æ–≥–∏—Ä—É–µ–º usage
    try:
        if hasattr(resp, "usage"):
            prompt_tokens = getattr(resp.usage, "prompt_tokens", None)
            completion_tokens = getattr(resp.usage, "completion_tokens", None)
            total_tokens = getattr(resp.usage, "total_tokens", None)
            logger.info(f"OCR —Ç–æ–∫–µ–Ω—ã ‚Äî prompt: {prompt_tokens}, completion: {completion_tokens}, total: {total_tokens}")
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å usage –∏–∑ –æ—Ç–≤–µ—Ç–∞: {e}")

    text = resp.choices[0].message.content or ""

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ GPT –≤—Å–µ –∂–µ –≤–µ—Ä–Ω—É–ª –ø–æ–ª–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
    if latex_mode:
        text = _strip_latex_fences(text)
        text = extract_content_from_latex(text)

    return text.strip()


CLS_INSTRUCTIONS = (
    "–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Ç–µ–∫—Å—Ç–∞ –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: "
    "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —Ç–µ–æ—Ä–µ–º–∞, –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ, –ø—Ä–∏–º–µ—Ä, —Ñ–æ—Ä–º—É–ª–∞, –≤–∞–∂–Ω—ã–π_—Ñ–∞–∫—Ç, –¥–∞—Ç–∞, –æ–±—â–∏–π_—Ç–µ–∫—Å—Ç.\n\n"
)


def classify_segment_with_gpt(segment: str, model: str = OPENAI_MODEL_CLS) -> str:
    prompt = f"{CLS_INSTRUCTIONS}–§—Ä–∞–≥–º–µ–Ω—Ç:\n\"\"\"\n{segment}\n\"\"\"\n–û—Ç–≤–µ—Ç –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º –Ω–∞ —Ä—É—Å—Å–∫–æ–º:"
    resp = chat_call_with_retry(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=10,
        temperature=0.0
    )
    return (resp.choices[0].message.content or "").strip().lower()


def convert_latex_to_html(text: str) -> str:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç LaTeX —Ç–µ–∫—Å—Ç –≤ HTML —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π MathJax
    """
    # –û—Å—Ç–∞–≤–ª—è–µ–º LaTeX –º–∞—Ç–µ–º–∞—Ç–∏–∫—É –∫–∞–∫ –µ—Å—Ç—å –¥–ª—è MathJax
    # –ó–∞–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã LaTeX –Ω–∞ HTML

    # –ó–∞–º–µ–Ω—è–µ–º LaTeX –æ–∫—Ä—É–∂–µ–Ω–∏—è –Ω–∞ HTML —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏
    html_text = text

    # –ó–∞–º–µ–Ω—è–µ–º \section{...} –Ω–∞ <h2>...</h2>
    html_text = re.sub(r'\\section\*?\{([^}]+)\}', r'<h2>\1</h2>', html_text)

    # –ó–∞–º–µ–Ω—è–µ–º \subsection{...} –Ω–∞ <h3>...</h3>
    html_text = re.sub(r'\\subsection\*?\{([^}]+)\}', r'<h3>\1</h3>', html_text)

    # –ó–∞–º–µ–Ω—è–µ–º \textbf{...} –Ω–∞ <strong>...</strong>
    html_text = re.sub(r'\\textbf\{([^}]+)\}', r'<strong>\1</strong>', html_text)

    # –ó–∞–º–µ–Ω—è–µ–º \textit{...} –Ω–∞ <em>...</em>
    html_text = re.sub(r'\\textit\{([^}]+)\}', r'<em>\1</em>', html_text)

    # –ó–∞–º–µ–Ω—è–µ–º –¥–≤–æ–π–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫ –Ω–∞ –∞–±–∑–∞—Ü—ã
    html_text = re.sub(r'\n\s*\n', '</p><p>', html_text)

    # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if not html_text.startswith('<'):
        html_text = '<p>' + html_text
    if not html_text.endswith('</p>'):
        html_text = html_text + '</p>'

    # –ó–∞–º–µ–Ω—è–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫ –Ω–∞ <br>
    html_text = re.sub(r'(?<!>)\n(?!<)', '<br>', html_text)

    return html_text


class GPTNoteAnalyzer:
    def split_into_segments(self, text: str) -> List[str]:
        return [s.text.strip() for s in nlp(text).sents if s.text.strip()]

    def analyze_text(self, text: str) -> List[ClassifiedSegment]:
        logger.info("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (GPT)...")
        segments = self.split_into_segments(text)
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}")
        results: List[ClassifiedSegment] = []
        pos = 0
        for seg in segments:
            cat = classify_segment_with_gpt(seg)
            results.append(ClassifiedSegment(
                text=seg,
                category=cat,
                confidence=1.0,
                start_pos=pos,
                end_pos=pos + len(seg)
            ))
            pos += len(seg) + 1
        return results

    def create_highlighted_html(self, text: str, segments: List[ClassifiedSegment]) -> str:
        # –°–Ω–∞—á–∞–ª–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º LaTeX –≤ HTML
        html_text = convert_latex_to_html(text)

        # –ú–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–∞ CSS –∫–ª–∞—Å—Å—ã
        category_classes = {
            "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ": "highlight-definition",
            "—Ç–µ–æ—Ä–µ–º–∞": "highlight-theorem",
            "–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ": "highlight-theorem",
            "–ø—Ä–∏–º–µ—Ä": "highlight-example",
            "—Ñ–æ—Ä–º—É–ª–∞": "highlight-formula",
            "–≤–∞–∂–Ω—ã–π_—Ñ–∞–∫—Ç": "highlight-important",
            "–¥–∞—Ç–∞": "highlight-date",
            "–æ–±—â–∏–π_—Ç–µ–∫—Å—Ç": ""
        }

        segments_sorted = sorted(segments, key=lambda x: x.start_pos, reverse=True)

        for segment in segments_sorted:
            css_class = category_classes.get(segment.category, "")
            if css_class:
                # –ò—â–µ–º —Ç–µ–∫—Å—Ç —Å–µ–≥–º–µ–Ω—Ç–∞ –≤ HTML –∏ –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ span
                segment_text = segment.text
                if segment_text in html_text:
                    highlighted_text = f'<span class="{css_class}" title="{segment.category.title()}">{segment_text}</span>'
                    html_text = html_text.replace(segment_text, highlighted_text, 1)

        return html_text

    def summary_text(self, src_text: str, segments: List[ClassifiedSegment]) -> Dict:
        counts = Counter(s.category for s in segments)

        categories = {
            "definitions": [s for s in segments if "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ" in s.category],
            "examples": [s for s in segments if "–ø—Ä–∏–º–µ—Ä" in s.category],
            "theorems": [s for s in segments if "—Ç–µ–æ—Ä–µ–º–∞" in s.category or "–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ" in s.category],
            "formulas": [s for s in segments if "—Ñ–æ—Ä–º—É–ª–∞" in s.category],
            "dates": [s for s in segments if "–¥–∞—Ç–∞" in s.category],
        }

        topics = self.extract_topics(src_text)

        return {
            "total_segments": len(segments),
            "total_chars": len(src_text),
            "categories": {
                "definitions": len(categories["definitions"]),
                "examples": len(categories["examples"]),
                "theorems": len(categories["theorems"]),
                "formulas": len(categories["formulas"]),
                "dates": len(categories["dates"])
            },
            "topics": topics,
            "segments_by_category": {k: [asdict(s) for s in v] for k, v in categories.items()}
        }

    def extract_topics(self, text: str) -> List[str]:
        topics = []

        math_keywords = {
            "–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è": "–ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ",
            "–∏–Ω—Ç–µ–≥—Ä–∞–ª": "–ò–Ω—Ç–µ–≥—Ä–∞–ª—ã",
            "—Ñ—É–Ω–∫—Ü–∏—è": "–§—É–Ω–∫—Ü–∏–∏",
            "—É—Ä–∞–≤–Ω–µ–Ω–∏–µ": "–£—Ä–∞–≤–Ω–µ–Ω–∏—è",
            "–º–∞—Ç—Ä–∏—Ü–∞": "–ú–∞—Ç—Ä–∏—Ü—ã",
            "–ø—Ä–µ–¥–µ–ª": "–ü—Ä–µ–¥–µ–ª—ã"
        }

        for keyword, topic in math_keywords.items():
            if keyword in text.lower() and topic not in topics:
                topics.append(topic)

        return topics[:5]


def _pdf_doc(out_path: str):
    return SimpleDocTemplate(
        out_path, pagesize=A4,
        rightMargin=20 * mm, leftMargin=20 * mm,
        topMargin=15 * mm, bottomMargin=15 * mm
    )


def _pdf_styles():
    styles = getSampleStyleSheet()
    styles["BodyText"].leading = 14
    return styles


CATEGORY_TITLES_RU = {
    "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è",
    "—Ç–µ–æ—Ä–µ–º–∞": "–¢–µ–æ—Ä–µ–º—ã",
    "–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ": "–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞",
    "–ø—Ä–∏–º–µ—Ä": "–ü—Ä–∏–º–µ—Ä—ã",
    "—Ñ–æ—Ä–º—É–ª–∞": "–§–æ—Ä–º—É–ª—ã",
    "–≤–∞–∂–Ω—ã–π_—Ñ–∞–∫—Ç": "–í–∞–∂–Ω—ã–µ —Ñ–∞–∫—Ç—ã",
    "–¥–∞—Ç–∞": "–î–∞—Ç—ã –∏ —Å–æ–±—ã—Ç–∏—è",
    "–æ–±—â–∏–π_—Ç–µ–∫—Å—Ç": "–û–±—â–∏–π —Ç–µ–∫—Å—Ç"
}


def export_study_pack_pdf(segments: List[ClassifiedSegment], out_path="study_pack.pdf") -> str:
    doc = _pdf_doc(out_path)
    styles = _pdf_styles()
    h1, h2, body = styles["Heading1"], styles["Heading2"], styles["BodyText"]

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
    by_cat = {}
    for s in segments:
        key = s.category.lower().strip()
        by_cat.setdefault(key, []).append(s.text.strip())

    order = [
        "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "—Ç–µ–æ—Ä–µ–º–∞", "–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ",
        "–ø—Ä–∏–º–µ—Ä", "—Ñ–æ—Ä–º—É–ª–∞", "–≤–∞–∂–Ω—ã–π_—Ñ–∞–∫—Ç",
        "–¥–∞—Ç–∞", "–æ–±—â–∏–π_—Ç–µ–∫—Å—Ç"
    ]

    story = [Paragraph("–£—á–µ–±–Ω—ã–π –∫–æ–Ω—Å–ø–µ–∫—Ç", h1), Spacer(1, 8)]

    for cat in order:
        items = by_cat.get(cat, [])
        if not items:
            continue
        story.append(Paragraph(CATEGORY_TITLES_RU.get(cat, cat.title()), h2))
        story.append(Spacer(1, 6))
        for t in items:
            story.append(Paragraph("‚Ä¢ " + t.replace("\n", "<br/>"), body))
            story.append(Spacer(1, 4))
        story.append(Spacer(1, 8))

    doc.build(story)
    logger.info(f"PDF —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {out_path}")
    return out_path


def generate_latex_document(segments: List[ClassifiedSegment], original_content: str = "") -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π LaTeX –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ —Å–µ–≥–º–µ–Ω—Ç–æ–≤"""

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    by_cat = {}
    for s in segments:
        key = s.category.lower().strip()
        by_cat.setdefault(key, []).append(s.text.strip())

    order = [
        "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "—Ç–µ–æ—Ä–µ–º–∞", "–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ",
        "–ø—Ä–∏–º–µ—Ä", "—Ñ–æ—Ä–º—É–ª–∞", "–≤–∞–∂–Ω—ã–π_—Ñ–∞–∫—Ç",
        "–¥–∞—Ç–∞", "–æ–±—â–∏–π_—Ç–µ–∫—Å—Ç"
    ]

    # –ù–∞—á–∞–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    latex_content = r"""\documentclass[12pt,a4paper]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}
\usepackage{xcolor}

% –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–≤–µ—Ç–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
\definecolor{definitionbg}{RGB}{220, 252, 231}
\definecolor{theorembg}{RGB}{254, 226, 226}
\definecolor{examplebg}{RGB}{254, 243, 199}
\definecolor{formulabg}{RGB}{207, 250, 254}
\definecolor{datebg}{RGB}{233, 213, 255}

\newcommand{\highlightdefinition}[1]{\colorbox{definitionbg}{#1}}
\newcommand{\highlighttheorem}[1]{\colorbox{theorembg}{#1}}
\newcommand{\highlightexample}[1]{\colorbox{examplebg}{#1}}
\newcommand{\highlightformula}[1]{\colorbox{formulabg}{#1}}
\newcommand{\highlightdate}[1]{\colorbox{datebg}{#1}}

\begin{document}

\title{–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Å–ø–µ–∫—Ç}
\author{ClassifyMyNotes AI}
\date{\today}
\maketitle

\tableofcontents
\newpage

"""

    # –ï—Å–ª–∏ –µ—Å—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ
    if original_content.strip():
        latex_content += r"""
\section{–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç}
""" + original_content + r"""

\newpage
"""

    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    latex_content += r"""
\section{–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —Ç–∏–ø–∞–º}

"""

    for cat in order:
        items = by_cat.get(cat, [])
        if not items:
            continue

        section_title = CATEGORY_TITLES_RU.get(cat, cat.title())
        latex_content += f"\\subsection{{{section_title}}}\n\n"

        for item in items:
            # –ù–µ —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º LaTeX –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º—É–ª
            if cat == "—Ñ–æ—Ä–º—É–ª–∞":
                latex_content += f"\\begin{{equation}}\n{item}\n\\end{{equation}}\n\n"
            else:
                # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, —Å–æ—Ö—Ä–∞–Ω—è—è LaTeX –∫–æ–º–∞–Ω–¥—ã
                escaped_item = item.replace("&", "\\&").replace("%", "\\%").replace("#", "\\#")
                latex_content += f"\\textbf{{‚Ä¢}} {escaped_item}\n\n"

    latex_content += r"""
\end{document}
"""
    return latex_content


# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
def extract_content_from_latex(text: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑ LaTeX –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    content_match = re.search(r'\\begin\{document\}(.*?)\\end\{document\}', text, re.DOTALL)
    if content_match:
        return content_match.group(1).strip()
    return clean_latex_for_document_body(text)


def clean_latex_for_document_body(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç LaTeX —Ç–µ–∫—Å—Ç –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ —Ç–µ–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    text = re.sub(r'\\documentclass(?:\[.*?\])?\{.*?\}', '', text)
    text = re.sub(r'\\usepackage(?:\[.*?\])?\{.*?\}', '', text)
    text = re.sub(r'\\geometry\{.*?\}', '', text)
    text = re.sub(r'\\begin\{document\}', '', text)
    text = re.sub(r'\\end\{document\}', '', text)
    text = re.sub(r'\n\s*\n', '\n\n', text)
    return text.strip()


def simplify_latex_content(content: str) -> str:
    """–£–ø—Ä–æ—â–∞–µ—Ç LaTeX –∫–æ–Ω—Ç–µ–Ω—Ç, —É–¥–∞–ª—è—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã"""
    content = re.sub(r'\\begin\{align\*?\}', r'\\begin{equation}', content)
    content = re.sub(r'\\end\{align\*?\}', r'\\end{equation}', content)
    content = re.sub(r'\\[a-zA-Z]+\*?\{[^}]*\}',
                     lambda m: m.group(0) if any(
                         cmd in m.group(0) for cmd in ['frac', 'sqrt', 'sum', 'int', 'text']) else '',
                     content)
    return content


def try_build_latex_pdf(tex_path: str, tmpdir: str, engines=("lualatex", "xelatex", "pdflatex"), passes=2, timeout=35):
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è —Å–æ–±—Ä–∞—Ç—å PDF –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –¥–≤–∏–∂–∫–∞–º–∏. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (pdf_path|None, error_text).
    –ù–ï –¥–µ–∫–æ–¥–∏—Ä—É–µ—Ç –≤—ã–≤–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ (text=False) ‚Äî —Å–∞–º–∏ –¥–µ–∫–æ–¥–∏—Ä—É–µ–º —Å errors='replace'.
    """
    last_err = ""
    # –°—Ç–∞–±–∏–ª—å–Ω–∞—è –ª–æ–∫–∞–ª—å –¥–ª—è –ø–æ–¥–ø—Ä–æ—Ü–µ—Å—Å–æ–≤
    env = os.environ.copy()
    env.setdefault("LANG", "en_US.UTF-8")
    env.setdefault("LC_ALL", "en_US.UTF-8")

    for eng in engines:
        try:
            ok = True
            for i in range(passes):
                result = subprocess.run(
                    [eng, "-interaction=nonstopmode", "-halt-on-error", "-file-line-error", tex_path],
                    cwd=tmpdir,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=False,
                    timeout=timeout,
                    check=False,
                    env=env,
                )
                stdout = (result.stdout or b"").decode("utf-8", errors="replace")
                stderr = (result.stderr or b"").decode("utf-8", errors="replace")

                if result.returncode != 0:
                    ok = False
                    last_err = f"[{eng} pass {i + 1}] rc={result.returncode}\n--- stdout ---\n{stdout}\n--- stderr ---\n{stderr}"
                    logger.error(last_err)
                    break

            if ok:
                pdf_path = os.path.join(tmpdir, "output.pdf")
                if os.path.exists(pdf_path):
                    logger.info(f"{eng}: PDF —É—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω: {pdf_path}")
                    return pdf_path, ""

        except subprocess.TimeoutExpired as te:
            last_err = f"{eng} timeout after {timeout}s"
            logger.error(last_err)
        except FileNotFoundError:
            last_err = f"{eng} not found"
            logger.error(last_err)
            continue
        except Exception as e:
            last_err = f"{eng} unexpected error: {e}"
            logger.exception(last_err)

    return None, last_err


_MATH_TRIGGERS = (
    r"\\int", r"\\frac", r"\\sum", r"\\prod", r"\\lim", r"\\sqrt",
    r"\\sin", r"\\cos", r"\\tan", r"\\log", r"\\ln",
    r"\\alpha", r"\\beta", r"\\gamma", r"\\infty", r"\\left", r"\\right",
)


def _strip_fake_single_letter_cmds(s: str) -> str:
    s = re.sub(r"\\([a-zA-Z])(?=\s*[\(\[\{])", r"\1", s)
    s = re.sub(r"\\([a-zA-Z])(\b)", r"\1\2", s)
    return s


def _line_needs_math(line: str) -> bool:
    if "$" in line or r"\[" in line or r"\(" in line:
        return False
    for p in _MATH_TRIGGERS:
        if re.search(p, line):
            return True
    if re.search(r"[\^_]\s*[\{\(]?", line):
        return True
    return False


def auto_wrap_display_math(text: str) -> str:
    r"""–°—Ç—Ä–æ–∫–∏ —Å LaTeX-—Ñ–æ—Ä–º—É–ª–∞–º–∏, –Ω–µ –æ–∫—Ä—É–∂—ë–Ω–Ω—ã–µ math, –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ \[...\]."""
    lines = text.splitlines()
    out = []
    for ln in lines:
        l = ln.strip()
        if _line_needs_math(l):
            out.append(r"\[")
            out.append(ln)
            out.append(r"\]")
        else:
            out.append(ln)
    return "\n".join(out)


def heal_latex_content(s: str) -> str:
    r"""
    –ë—ã—Å—Ç—Ä–∞—è ¬´–ø–æ–¥–ª–∞—Ç–∫–∞¬ª LaTeX –ø–æ—Å–ª–µ OCR:
    - —É–±–∏—Ä–∞–µ–º code-fences
    - –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º $...$
    - –∑–∞–∫—Ä—ã–≤–∞–µ–º \[ ... \] –µ—Å–ª–∏ –æ—Ç–∫—Ä—ã—Ç–∏–π –±–æ–ª—å—à–µ
    - –ª–µ—á–∏–º align ‚Üí equation –ø—Ä–∏ –±–∏—Ç—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö
    - –≤—ã—á–∏—â–∞–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –æ–ø–∞—Å–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
    - –£–î–ê–õ–Ø–ï–ú –º—É—Å–æ—Ä–Ω—ã–µ \f( –∏ –ø–æ–¥–æ–±–Ω—ã–µ
    """
    s = _strip_latex_fences(s or "")

    if s.count("$") % 2 == 1:
        s = s.replace("$", "")

    opens = len(re.findall(r"\\\[", s))
    closes = len(re.findall(r"\\\]", s))
    if opens > closes:
        s += ("\n" + "\\]") * (opens - closes)

    s = re.sub(r'\\begin\{align\*?\}([\s\S]*?)\\end\{align\*?\}', r'\\begin{equation}\1\\end{equation}', s)

    s = re.sub(r'\\(write|input|include)\b.*', '', s)

    s = _strip_fake_single_letter_cmds(s)

    return s


async def ask_llm_about_notes(context_text: str, question: str, model: str = OPENAI_MODEL_CLS) -> str:
    prompt = (
        "–ù–∏–∂–µ —É—á–µ–±–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É, "
        "—Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è/—Ç–µ–æ—Ä–µ–º—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –µ—Å–ª–∏ —É–º–µ—Å—Ç–Ω–æ.\n\n"
        f"–ö–û–ù–¢–ï–ö–°–¢:\n{safe_truncate(context_text)}\n\n–í–û–ü–†–û–°: {question}"
    )
    resp = chat_call_with_retry(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=600,
        temperature=0.2
    )
    return (resp.choices[0].message.content or "").strip()


def detect_mime(filename: str, declared: str | None) -> str:
    if declared:
        return declared
    guess, _ = mimetypes.guess_type(filename)
    return guess or "application/octet-stream"


@app.post("/upload")
async def upload_and_process(file: UploadFile):
    logger.info("=== –ù–ê–ß–ê–õ–û –û–ë–†–ê–ë–û–¢–ö–ò ===")
    logger.info("–ü–æ–ª—É—á–µ–Ω —Ñ–∞–π–ª: %s, —Ç–∏–ø: %s, —Ä–∞–∑–º–µ—Ä: %d –±–∞–π—Ç",
                file.filename, file.content_type, file.size if hasattr(file, "size") else -1)

    with tempfile.TemporaryDirectory() as tmpdir:
        file_path = os.path.join(tmpdir, file.filename)
        with open(file_path, "wb") as f:
            f.write(await file.read())

        extracted_text = ""
        if file.filename.lower().endswith(".pdf"):
            logger.info("PDF ‚Üí smart extract (—Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π + OCR-—Ñ–æ–ª–ª–±–µ–∫ –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ)‚Ä¶")
            extracted_text = await extract_pdf_text_smart(file_path)
        else:
            extracted_text = ""

        logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(extracted_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        logger.info(f"–ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤: {extracted_text[:500]}")

        analyzer = GPTNoteAnalyzer()
        segments = analyzer.analyze_text(extracted_text)
        highlighted_html = analyzer.create_highlighted_html(extracted_text, segments)
        summary = analyzer.summary_text(extracted_text, segments)

        if extracted_text.strip().startswith('\\documentclass') or '\\begin{document}' in extracted_text:
            document_content = extract_content_from_latex(extracted_text)
        else:
            document_content = extracted_text

        healed = heal_latex_content(document_content)
        healed = auto_wrap_display_math(healed)
        clean_content = fix_latex_commands(healed)

        full_latex_source = generate_latex_document(segments, clean_content)

        latex_template = r"""\documentclass[12pt,a4paper]{article}
        \usepackage{fontspec}
        \usepackage{polyglossia}
        \setdefaultlanguage{russian}
        \setotherlanguage{english}
        \defaultfontfeatures{Ligatures=TeX}
        \setmainfont{CMU Serif}
        \setsansfont{CMU Sans Serif}
        \setmonofont{CMU Typewriter Text}
        \usepackage{unicode-math}
        \setmathfont{Latin Modern Math}
        \usepackage{geometry}
        \geometry{margin=1in}
        \usepackage[colorlinks=true, linkcolor=blue]{hyperref}

        \begin{document}
        \title{–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç}
        \author{}
        \date{\today}
        \maketitle

        """ + clean_content + r"""

        \end{document}"""

        tex_path = os.path.join(tmpdir, "output.tex")
        with open(tex_path, "w", encoding='utf-8') as tex_file:
            tex_file.write(latex_template)

        pdf_path, build_err = try_build_latex_pdf(tex_path, tmpdir)

        pdf_b64 = None
        if pdf_path and os.path.exists(pdf_path):
            with open(pdf_path, "rb") as pdf_file:
                pdf_b64 = base64.b64encode(pdf_file.read()).decode("ascii")
        else:
            simplified_content = simplify_latex_content(clean_content)
            simplified_template = r"""\documentclass[12pt,a4paper]{article}
            \usepackage{fontspec}
            \usepackage{polyglossia}
            \setdefaultlanguage{russian}
            \defaultfontfeatures{Ligatures=TeX}
            \setmainfont{CMU Serif}
            \usepackage{geometry}
            \geometry{margin=1in}

            \begin{document}
            \section*{–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç}

            """ + simplified_content + r"""

            \end{document}"""

            with open(tex_path, "w", encoding='utf-8') as tex_file:
                tex_file.write(simplified_template)
            pdf_path2, build_err2 = try_build_latex_pdf(tex_path, tmpdir)
            if pdf_path2 and os.path.exists(pdf_path2):
                with open(pdf_path2, "rb") as pdf_file:
                    pdf_b64 = base64.b64encode(pdf_file.read()).decode("ascii")
            else:
                logger.error("LaTeX build failed, returning without pdf_base64.\nFirst error:\n%s\nSecond error:\n%s",
                             build_err, build_err2)

        logger.info("=== –ó–ê–í–ï–†–®–ï–ù–ò–ï –û–ë–†–ê–ë–û–¢–ö–ò ===")
        return {
            "success": True,
            "extracted_text": extracted_text,
            "segments": [asdict(s) for s in segments],
            "highlighted_html": highlighted_html,
            "summary": summary,
            "pdf_base64": pdf_b64,
            "latex_source": full_latex_source,
            "file_info": {
                "filename": file.filename,
                "original_text_length": len(extracted_text),
                "processed_text_length": len(clean_content),
                "segments_count": len(segments),
                "latex_build_ok": bool(pdf_b64)
            }
        }


@app.post("/chat")
async def chat_with_ai(message: ChatMessage):
    try:
        response = await ask_llm_about_notes(
            message.context or "",
            message.message
        )

        suggestions = [
            "–ú–æ–∂–µ—à—å –æ–±—ä—è—Å–Ω–∏—Ç—å —ç—Ç–æ –ø—Ä–æ—â–µ?",
            "–ü—Ä–∏–≤–µ–¥–∏ –µ—â–µ –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä",
            "–ö–∞–∫ —ç—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å –¥—Ä—É–≥–∏–º–∏ —Ç–µ–º–∞–º–∏?",
            "–ö–∞–∫–∏–µ –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è?"
        ]

        return ChatResponse(response=response, suggestions=suggestions)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞: {str(e)}")


@app.post("/export/pdf")
async def export_pdf(segments_data: List[Dict]):
    try:
        segments = [ClassifiedSegment(**data) for data in segments_data]

        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            pdf_path = export_study_pack_pdf(segments, tmp.name)

        return FileResponse(
            pdf_path,
            media_type="application/pdf",
            filename="–∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π_–∫–æ–Ω—Å–ø–µ–∫—Ç.pdf"
        )

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ PDF: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {str(e)}")


@app.post("/export/tex")
async def export_tex(segments_data: List[Dict]):
    """–ù–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ LaTeX —Ñ–∞–π–ª–∞"""
    try:
        segments = [ClassifiedSegment(**data) for data in segments_data]

        latex_content = generate_latex_document(segments)

        return PlainTextResponse(
            latex_content,
            media_type="text/plain; charset=utf-8",
            headers={"Content-Disposition": "attachment; filename=–∫–æ–Ω—Å–ø–µ–∫—Ç.tex"}
        )

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ LaTeX: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ LaTeX: {str(e)}")


@app.get("/health")
async def health_check():
    return {"status": "healthy", "version": "2.0.1", "models": {
        "ocr": OPENAI_MODEL_OCR,
        "classification": OPENAI_MODEL_CLS
    }}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)